{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **Reading the Data**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\n\n# Suppress specific types of warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n# Add more filterwarnings lines as needed to suppress other types of warnings\n\n# Your code goes here\ntd = pd.read_csv(\"/kaggle/input/sandeep-isi-ml4crypto/TrainingData.csv\")\ntd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"td[\"class\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"td.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"td.iloc[0,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = pd.read_excel(\"/kaggle/input/sandeep-isi-ml4crypto/Sample.xlsx\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into features (X) and labels (y)\nX = td['Bitstream'].apply(lambda x: [int(bit) for bit in x]).values\ny = td['class'].values\n\n# Convert X to a NumPy array\nX = pd.DataFrame(list(X)).values\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Training the models**","metadata":{}},{"cell_type":"markdown","source":"### Classical ML Models","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import TrainingArguments, Trainer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Logistic Regression**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n\n# Create a logistic regression model\nmodel = LogisticRegression()\n\n# Fit the model to the training data\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\nprint(\"Classification Report:\")\nprint(classification_rep)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **SVM**","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n\n# Create an SVM model\nmodel = SVC()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a callback function to monitor progress\ndef progress_callback(epoch, logs):\n    if epoch % 10 == 0:\n        # Make predictions on the test data\n        y_pred = model.predict(X_test)\n        # Calculate accuracy\n        accuracy = accuracy_score(y_test, y_pred)\n        print(f\"Epoch {epoch}, Accuracy: {accuracy}\")\n\n# Fit the model to the training data and monitor progress\nmodel.fit(X_train[:20000], y_train[:20000])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test data\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\nprint(\"Classification Report:\")\nprint(classification_rep)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **DTM**","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# Create a Decision Tree model\nmodel = DecisionTreeClassifier()\n\n# Fit the model to the training data\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\nprint(\"Classification Report:\")\nprint(classification_rep)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **XGBoost**","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\n# Create an XGBoost DMatrix for training and testing\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# Define the parameters for the XGBoost model\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',  # You can use other metrics like 'error' or 'auc'\n    'max_depth': 12,\n    'eta': 0.1,\n    'nrounds': 100,\n    'early_stopping_rounds': 10,\n    'verbose_eval': 10  # Print evaluation metrics every 10 rounds\n}\n\n# Train the XGBoost model and monitor progress\nwatchlist = [(dtrain, 'train'), (dtest, 'test')]\nmodel = xgb.train(params, dtrain, num_boost_round=params['nrounds'], evals=watchlist)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test data\ny_pred = model.predict(dtest)\n\n# Convert predicted probabilities to binary labels\ny_pred_binary = [1 if p >= 0.5 else 0 for p in y_pred]\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred_binary)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\nprint(\"Classification Report:\")\nprint(classification_rep)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **XGB Ensemble**","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n\n# Split the training data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.05, random_state=42)\n\n# Define the parameters for the XGBoost model\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',  # You can use other metrics like 'error' or 'logloss'\n    'max_depth': 20,\n    'eta': 0.1,\n    'n_estimators': 10,  # Number of trees in the ensemble\n    'early_stopping_rounds': 10,\n    'verbose': 10  # Print evaluation metrics every 10 rounds\n}\n\n# Create an XGBoost classifier\nmodel = XGBClassifier(**params)\n\n# Train the XGBoost model with early stopping\nevals = [(X_val, y_val)]  # Specify the validation dataset\nmodel.fit(X_train, y_train, eval_set=evals, verbose=True)\n\n# Make predictions on the test data\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\nprint(\"Classification Report:\")\nprint(classification_rep)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Deep Learning Models**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into features (X) and labels (y)\nX = td['Bitstream'].apply(lambda x: [int(bit) for bit in x]).values\ny = td['class'].values\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Multi-layer perceptron (ffn)**","metadata":{}},{"cell_type":"code","source":"# Split the dataset into features (X) and labels (y)\nX = td['Bitstream'].apply(lambda x: [int(bit) for bit in x]).values\ny = td['class'].values\n\n# Convert X to a NumPy array\nX = np.array(list(X))  # Convert the list of lists to a NumPy array\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05, random_state=42)\n\n# Define a simple feedforward neural network\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(64,)),  # Input layer with 64 features\n    tf.keras.layers.Dense(64, activation='relu'),  # Hidden layer with 64 neurons and ReLU activation\n    tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer with 128 neurons and ReLU activation\n    tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer with 128 neurons and ReLU activation\n    tf.keras.layers.Dense(256, activation='relu'),  # Additional hidden layer with 256 neurons and ReLU activation\n    tf.keras.layers.Dense(256, activation='relu'),  # Additional hidden layer with 256 neurons and ReLU activation\n    tf.keras.layers.Dense(256, activation='relu'),  # Additional hidden layer with 256 neurons and ReLU activation\n    tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer with 128 neurons and ReLU activation\n    tf.keras.layers.Dense(128, activation='relu'),  # Additional hidden layer with 128 neurons and ReLU activation\n    tf.keras.layers.Dense(64, activation='relu'),   # Additional hidden layer with 64 neurons and ReLU activation\n    tf.keras.layers.Dense(64, activation='relu'),   # Additional hidden layer with 64 neurons and ReLU activation\n    tf.keras.layers.Dense(2, activation='softmax')  # Output layer with 2 neurons and softmax activation for binary classification\n])\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nbatch_size = 3200\nepochs = 5\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the validation data\ny_val_pred = model.predict(X_val)\ny_val_pred = np.argmax(y_val_pred, axis=1)\n\naccuracy = accuracy_score(y_val, y_val_pred)\nconfusion = confusion_matrix(y_val, y_val_pred)\nclassification_rep = classification_report(y_val, y_val_pred)\n\nprint(\"Validation Accuracy: {:.2f}%\".format(accuracy * 100))\nprint(\"Confusion Matrix:\")\nprint(confusion)\nprint(\"Classification Report:\")\nprint(classification_rep)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **MLP - Reccurrent Neurons (LSTM)**","metadata":{}},{"cell_type":"code","source":"\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05, random_state=42)\n\n# Reshape data for LSTM layers\nX_train = X_train.reshape(X_train.shape[0], 64, 1)\nX_val = X_val.reshape(X_val.shape[0], 64, 1)\n\n# Define a neural network with LSTM layers\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(64, 1)),  # Input layer with 64 time steps and 1 feature\n    tf.keras.layers.LSTM(64, return_sequences=True),  # LSTM layer with 64 units and return sequences\n    tf.keras.layers.LSTM(128, return_sequences=True),  # LSTM layer with 128 units and return sequences\n    tf.keras.layers.Dense(256, activation='relu'),  # Additional dense layer with 256 neurons and ReLU activation\n    tf.keras.layers.LSTM(256, return_sequences=True),  # LSTM layer with 256 units and return sequences\n    tf.keras.layers.Dense(128, activation='relu'),  # Additional dense layer with 128 neurons and ReLU activation\n    tf.keras.layers.LSTM(64),  # LSTM layer with 64 units\n    tf.keras.layers.Dense(10, activation='softmax'),  # Output layer with 5 neurons and softmax activation for binary classification\n    tf.keras.layers.Dense(2, activation='sigmoid') \n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nbatch_size = 12800\nepochs = 5\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the validation data\ny_val_pred = model.predict(X_val)\ny_val_pred = np.argmax(y_val_pred, axis=1)\n\naccuracy = accuracy_score(y_val, y_val_pred)\nconfusion = confusion_matrix(y_val, y_val_pred)\nclassification_rep = classification_report(y_val, y_val_pred)\n\nprint(\"Validation Accuracy: {:.2f}%\".format(accuracy * 100))\nprint(\"Confusion Matrix:\")\nprint(confusion)\nprint(\"Classification Report:\")\nprint(classification_rep)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **CNN + LSTM**","metadata":{}},{"cell_type":"code","source":"# Define a neural network with LSTM and CNN layers\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(64, 1)),  # Input layer with 64 time steps and 1 feature\n    tf.keras.layers.Conv1D(64, 3, activation='relu'),  # 1D convolutional layer with 64 filters and a kernel size of 3\n    tf.keras.layers.MaxPooling1D(2),  # Max pooling layer\n    tf.keras.layers.LSTM(64, return_sequences=True),  # LSTM layer with 64 units and return sequences\n    tf.keras.layers.LSTM(128, return_sequences=True),  # LSTM layer with 128 units and return sequences\n    tf.keras.layers.Dense(256, activation='relu'),  # Additional dense layer with 256 neurons and ReLU activation\n    tf.keras.layers.LSTM(256, return_sequences=True),  # LSTM layer with 256 units and return sequences\n    tf.keras.layers.Dense(128, activation='relu'),  # Additional dense layer with 128 neurons and ReLU activation\n    tf.keras.layers.LSTM(64),  # LSTM layer with 64 units\n    tf.keras.layers.Dense(10, activation='softmax'),  # Output layer with 10 neurons and softmax activation for multi-class classification\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nbatch_size = 3200\nepochs = 5\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the validation data\ny_val_pred = model.predict(X_val)\ny_val_pred = np.argmax(y_val_pred, axis=1)\n\naccuracy = accuracy_score(y_val, y_val_pred)\nconfusion = confusion_matrix(y_val, y_val_pred)\nclassification_rep = classification_report(y_val, y_val_pred)\n\nprint(\"Validation Accuracy: {:.2f}%\".format(accuracy * 100))\nprint(\"Confusion Matrix:\")\nprint(confusion)\nprint(\"Classification Report:\")\nprint(classification_rep)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **CNN+LSTM - more layers**","metadata":{}},{"cell_type":"code","source":"# Define a neural network with LSTM and CNN layers\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(64, 1)),  # Input layer with 64 time steps and 1 feature\n    tf.keras.layers.Conv1D(64, 3, activation='relu'),  # 1D convolutional layer with 64 filters and a kernel size of 3\n    tf.keras.layers.MaxPooling1D(2),  # Max pooling layer\n    tf.keras.layers.LSTM(64, return_sequences=True),  # LSTM layer with 64 units and return sequences\n    tf.keras.layers.LSTM(128, return_sequences=True),  # LSTM layer with 128 units and return sequences\n    tf.keras.layers.Dense(256, activation='relu'),  # Additional dense layer with 256 neurons and ReLU activation\n    tf.keras.layers.LSTM(256, return_sequences=True),  # LSTM layer with 256 units and return sequences\n    tf.keras.layers.Dense(128, activation='relu'),  # Additional dense layer with 128 neurons and ReLU activation\n    tf.keras.layers.LSTM(64),  # LSTM layer with 64 units\n    tf.keras.layers.Dense(10, activation='softmax'),# Output layer with 10 neurons and softmax activation for multi-class classification\n    tf.keras.layers.Dense(2, activation='sigmoid'),# Output layer with 10 neurons and softmax activation for multi-class classification\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nbatch_size = 8000\nepochs = 5\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n\n# Evaluate the model on the validation data\ny_val_pred = model.predict(X_val)\ny_val_pred = np.argmax(y_val_pred, axis=1)\n\naccuracy = accuracy_score(y_val, y_val_pred)\nconfusion = confusion_matrix(y_val, y_val_pred)\nclassification_rep = classification_report(y_val, y_val_pred)\n\nprint(\"Validation Accuracy: {:.2f}%\".format(accuracy * 100))\nprint(\"Confusion Matrix:\")\nprint(confusion)\nprint(\"Classification Report:\")\nprint(classification_rep)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a neural network with LSTM and CNN layers\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(64, 1)),  # Input layer with 64 time steps and 1 feature\n    tf.keras.layers.Conv1D(64, 3, activation='relu'),  # 1D convolutional layer with 64 filters and a kernel size of 3\n    tf.keras.layers.MaxPooling1D(2),  # Max pooling layer\n    tf.keras.layers.LSTM(128, return_sequences=True),  # LSTM layer with 128 units and return sequences\n    tf.keras.layers.LSTM(128, return_sequences=True),  # LSTM layer with 128 units and return sequences\n    tf.keras.layers.LSTM(64, return_sequences=True),  # LSTM layer with 64 units and return sequences\n    tf.keras.layers.Flatten(),  # Flatten the output for the fully connected layers\n    tf.keras.layers.Dense(256, activation='relu'),  # Additional dense layer with 256 neurons and ReLU activation\n    tf.keras.layers.Dense(128, activation='relu'),  # Additional dense layer with 128 neurons and ReLU activation\n    tf.keras.layers.Dense(2, activation='softmax'),  # Output layer with 2 neurons and softmax activation for binary classification\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nbatch_size = 6400\nepochs = 6\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n\n# Evaluate the model on the validation data\ny_val_pred = model.predict(X_val)\ny_val_pred = np.argmax(y_val_pred, axis=1)\n\naccuracy = accuracy_score(y_val, y_val_pred)\nconfusion = confusion_matrix(y_val, y_val_pred)\nclassification_rep = classification_report(y_val, y_val_pred, target_names=['Class 0', 'Class 1'])\n\nprint(\"Validation Accuracy: {:.2f}%\".format(accuracy * 100))\nprint(\"Confusion Matrix:\")\nprint(confusion)\nprint(\"Classification Report:\")\nprint(classification_rep)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **BERT**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**further**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nimport tensorflow as tf\n\n# Split the training data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.05, random_state=42)\n\n# Define a custom loss function for weighted average precision\ndef weighted_average_precision(y_true, y_pred):\n    # Define weights for positive and negative examples\n    weight_pos = 6265  # Adjust this value as needed\n    weight_neg = 6235  # Adjust this value as needed\n    \n    # Split y_true and y_pred into positive and negative examples\n    y_true_pos = tf.boolean_mask(y_true, tf.math.equal(y_true, 1))\n    y_pred_pos = tf.boolean_mask(y_pred, tf.math.equal(y_true, 1))\n    y_true_neg = tf.boolean_mask(y_true, tf.math.equal(y_true, 0))\n    y_pred_neg = tf.boolean_mask(y_pred, tf.math.equal(y_true, 0))\n    \n    # Calculate precision for positive and negative examples\n    precision_pos = tf.reduce_sum(tf.math.multiply(y_true_pos, y_pred_pos)) / (tf.reduce_sum(y_pred_pos) + 1e-7)\n    precision_neg = tf.reduce_sum(tf.math.multiply(y_true_neg, y_pred_neg)) / (tf.reduce_sum(y_pred_neg) + 1e-7)\n    \n    # Calculate weighted average precision\n    weighted_avg_precision = (weight_pos * precision_pos + weight_neg * precision_neg)\n    \n    return 1 - weighted_avg_precision\n\n# Define a neural network with LSTM and CNN layers\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(64, 1)),  # Input layer with 64 time steps and 1 feature\n    tf.keras.layers.Conv1D(64, 3, activation='relu'),  # 1D convolutional layer with 64 filters and a kernel size of 3\n    tf.keras.layers.MaxPooling1D(2),  # Max pooling layer\n    tf.keras.layers.Conv1D(64, 3, activation='relu'),  # 1D convolutional layer with 64 filters and a kernel size of 5\n    tf.keras.layers.MaxPooling1D(2),  # Max pooling layer\n    tf.keras.layers.LSTM(64, return_sequences=True),  # LSTM layer with 64 units and return sequences\n    tf.keras.layers.LSTM(128, return_sequences=True),  # LSTM layer with 128 units and return sequences\n    tf.keras.layers.Dense(256, activation='relu'),  # Additional dense layer with 256 neurons and ReLU activation\n    tf.keras.layers.LSTM(256, return_sequences=True),  # LSTM layer with 256 units and return sequences\n    tf.keras.layers.Dense(128, activation='relu'),  # Additional dense layer with 128 neurons and ReLU activation\n    tf.keras.layers.LSTM(64),  # LSTM layer with 64 units\n    tf.keras.layers.Dense(1, activation='sigmoid'),  # Output layer with sigmoid activation for binary classification\n])\n\n# Compile the model with the custom loss function\nmodel.compile(optimizer='adam', loss=weighted_average_precision, metrics=['accuracy'])\n\n# Train the model\nbatch_size = 3200\nepochs = 5\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n\n# Evaluate the model on the validation data\ny_val_pred = model.predict(X_val)\ny_val_pred = (y_val_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n\naccuracy = accuracy_score(y_val, y_val_pred)\nconfusion = confusion_matrix(y_val, y_val_pred)\nclassification_rep = classification_report(y_val, y_val_pred)\n\nprint(\"Validation Accuracy: {:.2f}%\".format(accuracy * 100))\nprint(\"Confusion Matrix:\")\nprint(confusion)\nprint(\"Classification Report:\")\nprint(classification_rep)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}